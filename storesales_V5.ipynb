{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de um modelo de machine learning para prever as vendas\n",
    "\n",
    "Temos 4 datasets, as transações para treino e para teste, as lojas, preços do óleo ao longo do tempo e os feriados.\n",
    "\n",
    "\n",
    "Link do dataset\n",
    "https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Coletando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando algumas bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(database='storesales', host='localhost', user='postgres', password='pokemonn123', port=5432 )\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT sl.date, sl.sales, sl.family, sl.onpromotion, sl.store_nbr, ol.dcoilwtico, st.city, st.state, st.cluster, st.type, tr.transactions\n",
    "FROM sales sl\n",
    "LEFT OUTER JOIN oil_prices ol\n",
    "ON sl.date = ol.date\n",
    "LEFT OUTER JOIN stores st\n",
    "ON sl.store_nbr = st.store_nbr\n",
    "LEFT OUTER JOIN transactions tr\n",
    "ON sl.date = tr.date AND sl.store_nbr = tr.store_nbr\n",
    "ORDER BY sl.date;\"\"\")\n",
    "rows = cur.fetchall()\n",
    "col_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "#for row in rows:\n",
    "#    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=rows, columns=col_names)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos utilizar o dataset de feriados no equador para saber se determinado dia foi feriado\n",
    "\n",
    "conn = psycopg2.connect(database='storesales', host='localhost', user='postgres', password='pokemonn123', port=5432 )\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT  *\n",
    "FROM holiday_events;\n",
    "\"\"\")\n",
    "rows = cur.fetchall()\n",
    "col_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hol = pd.DataFrame(data=rows, columns=col_names)\n",
    "#df_hol.set_index('date',inplace=True)\n",
    "df_hol['date'] = pd.to_datetime(df_hol['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hol.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull()]['dcoilwtico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dcoilwtico'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dcoilwtico'] = df['dcoilwtico'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dcoilwtico'] = df['dcoilwtico'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dcoilwtico'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_transactions = df['transactions'].median()\n",
    "df['transactions'] = df['transactions'].fillna(median_transactions)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver que existem feriados nacionais, regionais (estaduais) e municipais\n",
    "# Também pode haver feriados prolongados, ou o feriado pode ser transferido para outro dia\n",
    "df_hol.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vamos fazer um algoritmo para saber se determinado dia foi feriado e adicionar ao dataset\n",
    "df['is_holiday'] = 0\n",
    "for hol_index in df_hol[(df_hol['transferred']=='False') & (df_hol['hol_type'] != 'Work Day')].index:\n",
    "    #if data_hol == data:\n",
    "    holiday_date = df_hol.loc[hol_index, 'date']\n",
    "    if holiday_date in df['date'].values:\n",
    "        for i in  df[df['date']==holiday_date].index:\n",
    "            if df_hol.loc[hol_index, 'locale'] == 'National':\n",
    "                df.loc[i, 'is_holiday'] = 1\n",
    "                \n",
    "            elif (df_hol.loc[hol_index, 'locale'] == 'Regional') & (df_hol.loc[hol_index, 'locale_name'] == df.loc[i, 'state']):\n",
    "                df.loc[i, 'is_holiday'] = 1\n",
    "\n",
    "            elif (df_hol.loc[hol_index, 'locale']=='Local') & (df_hol.loc[hol_index, 'locale_name'] == df.loc[i, 'city']):\n",
    "                df.loc[i,'is_holiday'] = 1\n",
    "    \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['store_nbr', 'family', 'date'])\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['store_nbr','family'])['sales'].shift(1).rolling(window=7).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de lag_features:\n",
    "df['lag_1'] = df.groupby(['store_nbr','family'])['sales'].shift(1)\n",
    "df['lag_7'] = df.groupby(['store_nbr','family'])['sales'].shift(7)\n",
    "df['sales_roll_mean_7'] = df.groupby(['store_nbr','family'])['sales'].shift(1).rolling(window=7).mean()\n",
    "df['sales_roll_std_7'] = df.groupby(['store_nbr','family'])['sales'].shift(1).rolling(window=7).std()\n",
    "df['promo_last_7'] = df.groupby(['store_nbr','family'])['onpromotion'].shift(1).rolling(window=7).sum()\n",
    "\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "categorical_cols = ['family', 'city', 'state', 'type', 'cluster']\n",
    "\n",
    "# Encode each categorical column\n",
    "for col in categorical_cols:\n",
    "    \n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    print(f\"Encoded '{col}' column.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os dados:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "numerical_features = ['onpromotion', 'dcoilwtico', 'lag_1', 'lag_7', 'sales_roll_mean_7', 'sales_roll_std_7' ,'promo_last_7', 'transactions']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o DataFrame como CSV\n",
    "\n",
    "df.to_csv('df_3.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o CSV:\n",
    "df = pd.read_csv('df_3.csv', parse_dates=True, index_col='date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = df['sales'].to_numpy().astype('float32')\n",
    "#sales = np.log1p(sales)\n",
    "raw_data = df.drop(columns=['sales', 'date']).astype('float32').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales.shape)\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Pré Processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = int(0.75 * len(raw_data))\n",
    "num_val_samples = int(0.125 * len(raw_data))\n",
    "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
    "\n",
    "print(\"num_train_samples:\", num_train_samples)\n",
    "print(\"num_val_samples:\", num_val_samples)\n",
    "print(\"num_test_samples:\", num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "#from sklearn.model_selection import learning_curve\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dividindo o Dataset em features e label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos dividir o nosso dataset em treino e teste\n",
    "# Os dados de teste serão os últimos 15 dias\n",
    "#train_dataset = df_pos.loc[:'2017-07-31']\n",
    "#test_dataset = df_pos.loc['2017-08-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora iremos dividir os dados em features e labels:\n",
    "\n",
    "X_train = raw_data[: num_train_samples, :]\n",
    "y_train = sales[ : num_train_samples]\n",
    "\n",
    "X_val = raw_data [num_train_samples : num_train_samples + num_val_samples,  : ]\n",
    "y_val = sales[num_train_samples : num_train_samples + num_val_samples]\n",
    "\n",
    "X_test = raw_data[num_train_samples + num_val_samples: , :]\n",
    "y_test = sales[num_train_samples + num_val_samples : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Será calculado um modelo base, sendo que este irá admitir que as vendas futuras serão iguais as vendas da última semana (7 dias atrás)\n",
    "sales_df = pd.DataFrame({'sales':y_test})\n",
    "sales_df['7_days'] = sales_df['sales'].shift(1782 * 7)\n",
    "sales_df = sales_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', mean_absolute_error(sales_df['sales'], sales_df['7_days']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Regrssão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = lr.predict(X_val)\n",
    "print(f\"O valor do MAE é de: {mean_absolute_error(y_val, predicts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Modelo SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = sgd.predict(X_val)\n",
    "print('MAE:', mean_absolute_error(y_val, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Modelo RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = rf.predict(X_val)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_val, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Criando Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo com XGBoost\n",
    "\n",
    "clf = xgb.XGBRegressor(base_score=0.5, booster='gbtree',n_estimators=1000,objective='reg:squarederror', eval_metric='mae')\n",
    "\n",
    "history = clf.fit(X_train, y_train ,eval_set=[(X_train, y_train), (X_val, y_val)])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando a curva de validação\n",
    "train_scores = history.evals_result()['validation_0']['mae']\n",
    "val_scores = history.evals_result()['validation_1']['mae']\n",
    "#history.evals_result()\n",
    "epochs = len(val_scores)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1, epochs + 1), train_scores, 'r', label='Train MAE')\n",
    "plt.plot(range(1, epochs + 1), val_scores, 'b', label='Validation MAE')\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O modelo está sofrendo de overfitting, agora será utilizado um gridsearch para encontrar os melhores parâmetros para o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos usar early stopping para melhorar o overfitting:\n",
    "# Treinando o modelo com XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "clf = XGBRegressor(base_score=0.5, booster='gbtree',n_estimators=1000,objective='reg:squarederror',eval_metric='mae', early_stopping_rounds=50)\n",
    "\n",
    "history = clf.fit(X_train, y_train ,eval_set=[(X_train, y_train), (X_val, y_val)], verbose=True)\n",
    "\n",
    "print('Melhor score: ', history.best_score)\n",
    "print('Melhor iteração: ',history.best_iteration)\n",
    "\n",
    "# Plotando a curva de validação\n",
    "train_scores = history.evals_result()['validation_0']['mae']\n",
    "val_scores = history.evals_result()['validation_1']['mae']\n",
    "#history.evals_result()\n",
    "epochs = len(val_scores)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1, epochs + 1), train_scores, 'r', label='Train MAE')\n",
    "plt.plot(range(1, epochs + 1), val_scores, 'b', label='Validation MAE')\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "(max_depth, eta, subsample, reg_lambda)\n",
    "for max_depth in [2, 4, 6]\n",
    "for eta in [0.01, 0.05, 0.1]\n",
    "for subsample in [0.4, 0.6, 1]\n",
    "for reg_lambda in [0, 0.5, 10]\n",
    "]\n",
    "mae_min = float(\"Inf\")\n",
    "best_params=None\n",
    "\n",
    "for max_depth, eta, subsample, reg_lambda in gridsearch_params:\n",
    "\n",
    "    # Atualiza os parâmetros:\n",
    "    #params['max_depth'] = max_depth\n",
    "    #params['eta'] = eta\n",
    "    #params['subsample'] = subsample\n",
    "\n",
    "    clf = XGBRegressor(base_score=0.5, booster='gbtree',n_estimators=1000,objective='reg:squarederror',eval_metric='mae', early_stopping_rounds=50, max_depth=max_depth, eta=eta, subsample=subsample, reg_lambda=reg_lambda)\n",
    "    history = clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=100)\n",
    "    #train_scores = history.evals_result()['validation_0']['mae']\n",
    "    #val_scores = history.evals_result()['validation_1']['mae']\n",
    "\n",
    "    # Atualiza os melhores scores:\n",
    "    mae = history.best_score\n",
    "    if mae < mae_min:\n",
    "        mae_min = mae\n",
    "        best_params = (max_depth, eta, subsample, reg_lambda)\n",
    "\n",
    "print(f'Melhores parâmetros (max_depth, eta, subsample e reg_lambda): {best_params[0]}, {best_params[1]}, {best_params[2]}, {best_params[3]}, e MAE: {mae_min} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBRegressor(base_score=0.5, booster='gbtree',n_estimators=1000,objective='reg:squarederror',eval_metric='mae', early_stopping_rounds=50, max_depth=6, eta=0.05, subsample=1,\n",
    "                       gamma=0, reg_lambda=10)\n",
    "history = clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=100)\n",
    "\n",
    "print('Melhor score: ', history.best_score)\n",
    "print('Melhor iteração: ',history.best_iteration)\n",
    "\n",
    "# Plotando a curva de validação\n",
    "train_scores = history.evals_result()['validation_0']['mae']\n",
    "val_scores = history.evals_result()['validation_1']['mae']\n",
    "#history.evals_result()\n",
    "epochs = len(val_scores)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1, epochs + 1), train_scores, 'r', label='Train MAE')\n",
    "plt.plot(range(1, epochs + 1), val_scores, 'b', label='Validation MAE')\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=history.feature_importances_, index=df.drop(columns='sales').columns, columns=['importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando a importancia das features:\n",
    "f1 = pd.DataFrame(data=history.feature_importances_, index=df.drop(columns='sales').columns, columns=['importance'])\n",
    "f1 = f1.sort_values('importance', ascending=False)\n",
    "f1[:20].plot(kind='barh', title='Feature Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Criando Modelo com Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape= (82,), name='my_input')\n",
    "features = layers.Dense(128, activation = 'relu', name='feature_1')(inputs)\n",
    "features = layers.Dense(128, activation = 'relu', name='feature_2')(features)\n",
    "outputs = layers.Dense(1, name='output')(features) \n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss=['mean_squared_error'], metrics=['mae'])\n",
    "model.fit(X_treino, y_treino)\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de rede neural com 2 camadas, 128 unidades por camada e batch size de 1024\n",
    "# Será usado Blocking Time Series Split para Validação Cruzada\n",
    "# Plotando a Learning Curve com Blocking Time Series Split\n",
    "\n",
    "n_epochs= 100\n",
    "\n",
    "inputs = keras.Input(shape= (83,), name='my_input')\n",
    "features1 = layers.Dense(128, activation = 'relu', name='feature_1')(inputs)\n",
    "features2 = layers.Dense(128, activation = 'relu', name='feature_2')(features1)\n",
    "outputs = layers.Dense(1, name='output')(features2) \n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs= n_epochs, validation_data=(X_val, y_val), batch_size= 1024, verbose=True)\n",
    "\n",
    "mae_history_val = history.history['val_mae']\n",
    "mae_history_train = history.history['mae']\n",
    "loss_history_train = history.history['loss']\n",
    "loss_history_val = history.history['val_loss']\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Plotando o MAE do treinamento e da validação\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(range(1, len(mae_history_val) + 1), mae_history_val,'b' ,label='Validação')\n",
    "plt.plot(range(1, len(mae_history_train) + 1), mae_history_train, 'r--' ,label='Treinamento')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotando o Loss do treinamento e da validação\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(range(1, len(loss_history_val) + 1), loss_history_val,'b' ,label='Validação')\n",
    "plt.plot(range(1, len(loss_history_train) + 1), loss_history_train, 'r--' ,label='Treinamento')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Será criado mais camadas e mais unidades por camada e também será adicionado dropout para regularização\n",
    "\n",
    "callback_list = [keras.callbacks.ModelCheckpoint(filepath= 'sales_teste.keras', monitor='val_mae', save_best_only=True)]\n",
    "n_epochs= 100\n",
    "inputs = keras.Input(shape= (83,), name='my_input')\n",
    "features1 = layers.Dense(512, activation = 'relu', name='feature_1')(inputs)\n",
    "features1 = layers.Dropout(0.5)(features1)\n",
    "features2 = layers.Dense(512, activation = 'relu', name='feature_2')(features1)\n",
    "features2 = layers.Dropout(0.5)(features2)\n",
    "features3 = layers.Dense(512, activation = 'relu', name='feature_3')(features2)\n",
    "features3 = layers.Dropout(0.5)(features3)\n",
    "features4 = layers.Dense(512, activation = 'relu', name='feature_4')(features3)\n",
    "features4 = layers.Dropout(0.5)(features4)\n",
    "features5 = layers.Dense(512, activation='relu', name='feature_5')(features4)\n",
    "features5 = layers.Dropout(0.5)(features5)\n",
    "outputs = layers.Dense(1, name='output')(features5) \n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.0005), loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs= n_epochs, validation_data=(X_val, y_val), batch_size= 1024, callbacks=callback_list)\n",
    "\n",
    "\n",
    "\n",
    "mae_history_val = history.history['val_mae']\n",
    "mae_history_train = history.history['mae']\n",
    "loss_history_train = history.history['loss']\n",
    "loss_history_val = history.history['val_loss']\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Plotando o MAE do treinamento e da validação\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(range(1, len(mae_history_val) + 1), mae_history_val,'b' ,label='Validação')\n",
    "plt.plot(range(1, len(mae_history_train) + 1), mae_history_train, 'r--' ,label='Treinamento')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotando o Loss do treinamento e da validação\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(range(1, len(loss_history_val) + 1), loss_history_val,'b' ,label='Validação')\n",
    "plt.plot(range(1, len(loss_history_train) + 1), loss_history_train, 'r--' ,label='Treinamento')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"sales_teste.keras\")\n",
    "print(f\"Test MAE: {model.evaluate(X_val, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Criando os modelos finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora será criado os modelos finais para o algoritmo XGBoost e para a Rede neural, de acordo com os modelos criados anteiormente\n",
    "Os modelos agora utilizarão todo o dataset de treino e validação para treinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data.shape)\n",
    "print(sales.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desnormalizando o dataset\n",
    "raw_data = (raw_data * std) + mean\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Normalizando os dados novamente, mas agora levando em conta os dados de treino e validação\n",
    "mean = raw_data[:num_train_samples + num_val_samples].mean(axis=0)\n",
    "std = raw_data[:num_train_samples + num_val_samples].std(axis=0)\n",
    "raw_data = (raw_data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = raw_data[: num_train_samples + num_val_samples, :]\n",
    "y_train = sales[ : num_train_samples + num_val_samples]\n",
    "\n",
    "\n",
    "X_test = raw_data[num_train_samples + num_val_samples: , :]\n",
    "y_test = sales[num_train_samples + num_val_samples : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', mean_absolute_error(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Modelo final de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBRegressor(base_score=0.5, booster='gbtree',n_estimators=1000,objective='reg:squarederror',eval_metric='mae', early_stopping_rounds=50, max_depth=6, eta=0.05, subsample=1,\n",
    "                       gamma=0, reg_lambda=10)\n",
    "history = clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=100)\n",
    "\n",
    "print('Melhor score: ', history.best_score)\n",
    "print('Melhor iteração: ',history.best_iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Modelo final da Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Será criado mais camadas e mais unidades por camada e também será adicionado dropout para regularização\n",
    "\n",
    "callback_list = [keras.callbacks.ModelCheckpoint(filepath= 'sales_final.keras', monitor='val_mae', save_best_only=True)]\n",
    "n_epochs= 100\n",
    "inputs = keras.Input(shape= (83,), name='my_input')\n",
    "features1 = layers.Dense(512, activation = 'relu', name='feature_1')(inputs)\n",
    "features1 = layers.Dropout(0.5)(features1)\n",
    "features2 = layers.Dense(512, activation = 'relu', name='feature_2')(features1)\n",
    "features2 = layers.Dropout(0.5)(features2)\n",
    "features3 = layers.Dense(512, activation = 'relu', name='feature_3')(features2)\n",
    "features3 = layers.Dropout(0.5)(features3)\n",
    "features4 = layers.Dense(512, activation = 'relu', name='feature_4')(features3)\n",
    "features4 = layers.Dropout(0.5)(features4)\n",
    "features5 = layers.Dense(512, activation='relu', name='feature_5')(features4)\n",
    "features5 = layers.Dropout(0.5)(features5)\n",
    "outputs = layers.Dense(1, name='output')(features5) \n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.0005), loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs= n_epochs, validation_data=(X_test, y_test), batch_size= 1024, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"sales_final.keras\")\n",
    "print(f\"Test MAE: {model.evaluate(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
